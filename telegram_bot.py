import logging
import json
import random
import torch
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
from transformers import AutoModelForCausalLM, AutoTokenizer

# üî• Telegram Token (–∑–∞–º–µ–Ω–∏ –Ω–∞ —Å–≤–æ–π)
TOKEN = "7588087338:AAGesdDnSAWFW4zHsioSEckDovg92-PXmeA"

# üèéÔ∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º GPU (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
device = "cuda" if torch.cuda.is_available() else "cpu"

# üöÄ –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤—É—é —Ä—É—Å—Å–∫—É—é GPT-–º–æ–¥–µ–ª—å
model_name = "ai-forever/mGPT"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name).to(device)

# ‚ö° –£—Å–∫–æ—Ä—è–µ–º –º–æ–¥–µ–ª—å (–µ—Å–ª–∏ –µ—Å—Ç—å –≤–∏–¥–µ–æ–∫–∞—Ä—Ç–∞)
if device == "cuda":
    model.half()

# üîç –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# üìÇ –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø–∞–º—è—Ç—å—é (—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞)
def save_chat_history(user_id, chat_history):
    with open(f"history_{user_id}.json", "w") as file:
        json.dump(chat_history, file)

def load_chat_history(user_id):
    try:
        with open(f"history_{user_id}.json", "r") as file:
            return json.load(file)
    except FileNotFoundError:
        return []

# üèÅ –ö–æ–º–∞–Ω–¥–∞ /start
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("–ü—Ä–∏–≤–µ—Ç! –Ø —Ç–≤–æ–π AI-–±–æ—Ç. –ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å? üòä")

# ‚ÑπÔ∏è –ö–æ–º–∞–Ω–¥–∞ /help
async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("–Ø –º–æ–≥—É –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ —Ç–≤–æ–∏ –≤–æ–ø—Ä–æ—Å—ã! –ü—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏ –º–Ω–µ üòä")

# üîÑ –ö–æ–º–∞–Ω–¥–∞ /reset (–æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏)
async def reset_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.message.chat_id
    context.user_data["chat_history"] = []
    save_chat_history(user_id, [])
    await update.message.reply_text("–ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞ –æ—á–∏—â–µ–Ω–∞! –ù–∞—á–Ω–µ–º —Å —á–∏—Å—Ç–æ–≥–æ –ª–∏—Å—Ç–∞. üßπ")

# ü§£ –ö–æ–º–∞–Ω–¥–∞ /joke (—à—É—Ç–∫–∏)
async def joke_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    jokes = [
        "–ü–æ—á–µ–º—É –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç—ã –ª—é–±—è—Ç —Ç—ë–º–Ω—É—é —Ç–µ–º—É? –ü–æ—Ç–æ–º—É —á—Ç–æ —Å–≤–µ—Ç –ø—Ä–∏—Ç—è–≥–∏–≤–∞–µ—Ç –±–∞–≥–∏! ü§£",
        "–ß—Ç–æ —Å–∫–∞–∑–∞–ª Python –∫–æ–¥—É? '–¢—ã —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–µ–Ω!' üêç"
    ]
    await update.message.reply_text(random.choice(jokes))

# ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ GPT
def generate_response(chat_history):
    input_text = " ".join(chat_history)
    input_ids = tokenizer.encode(input_text, return_tensors="pt").to(device)

    output = model.generate(
        input_ids,
        max_length=150,  # –î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞
        temperature=0.8,  # –ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –ª–æ–≥–∏–∫–æ–π –∏ –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å—é
        top_p=0.9,  # –£–º–Ω–æ–µ —Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
        repetition_penalty=1.2,  # –£–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )

    bot_response = tokenizer.decode(output[:, input_ids.shape[-1]:][0], skip_special_tokens=True)

    # –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π - –¥–∞—ë–º –∑–∞—Ä–∞–Ω–µ–µ –∑–∞–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π
    if bot_response.strip() == "" or len(bot_response) < 10:
        bot_response = random.choice([
            "–•–º... –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π –≤–æ–ø—Ä–æ—Å! ü§î",
            "–î–∞–≤–∞–π –æ–±—Å—É–¥–∏–º! üòâ",
            "–ù–µ —Å–æ–≤—Å–µ–º –ø–æ–Ω—è–ª, –º–æ–∂–µ—à—å —É—Ç–æ—á–Ω–∏—Ç—å? üòä",
            "–•–æ—Ä–æ—à–∏–π –≤–æ–ø—Ä–æ—Å! –°–µ–π—á–∞—Å –ø–æ–¥—É–º–∞—é... ü§ì"
        ])

    return bot_response

# üì© –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π
async def respond(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_message = update.message.text.strip()
    user_id = update.message.chat_id
    logger.info(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å ({user_id}): {user_message}")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    chat_history = load_chat_history(user_id)
    chat_history.append(user_message)

    # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 —Å–æ–æ–±—â–µ–Ω–∏–π
    chat_history = chat_history[-10:]
    context.user_data["chat_history"] = chat_history

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç GPT
    bot_response = generate_response(chat_history)
    logger.info(f"–ë–æ—Ç: {bot_response}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞
    save_chat_history(user_id, chat_history)

    await update.message.reply_text(bot_response)

# üöÄ –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞
def main():
    application = Application.builder().token(TOKEN).build()

    # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–º–∞–Ω–¥—ã
    application.add_handler(CommandHandler("start", start))
    application.add_handler(CommandHandler("help", help_command))
    application.add_handler(CommandHandler("reset", reset_command))
    application.add_handler(CommandHandler("joke", joke_command))

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ–±—ã—á–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, respond))

    # –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞
    application.run_polling()

if __name__ == "__main__":
    main()
